{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uucGSHMWEhq6"
      },
      "outputs": [],
      "source": [
        "#IMPORTS\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from enum import Enum\n",
        "from functools import reduce\n",
        "from typing import Any, List\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from prettytable import PrettyTable as PrettyTable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "QNURnAJQEhq_"
      },
      "outputs": [],
      "source": [
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "# Define the file path\n",
        "file_path = '/content/drive/My Drive/all_data_loaded.csv'\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the DataFrame to verify it was loaded correctly\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJiR2bMZEsXY",
        "outputId": "4f3df229-b801-40c3-baa8-36ddd5c0b5ad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "              datetime        time fsym tsym       close        high  \\\n",
            "0  2015-01-01 00:00:00  1420070400  BTC  USD  320.999986  320.999986   \n",
            "1  2015-01-01 00:01:00  1420070460  BTC  USD  320.999986  320.999986   \n",
            "2  2015-01-01 00:02:00  1420070520  BTC  USD  320.999986  320.999986   \n",
            "3  2015-01-01 00:03:00  1420070580  BTC  USD  320.999986  320.999986   \n",
            "4  2015-01-01 00:04:00  1420070640  BTC  USD  320.969986  320.999986   \n",
            "\n",
            "          low        open  volumefrom  volumeto  \n",
            "0  320.999986  320.999986    1.736972    557.57  \n",
            "1  320.999986  320.999986    0.000000      0.00  \n",
            "2  320.999986  320.999986    1.413333    453.68  \n",
            "3  320.999986  320.999986    2.000000    642.00  \n",
            "4  320.969986  320.999986    2.843880    912.71  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.datetime = pd.to_datetime(df.datetime)\n",
        "df.set_index(df.datetime, inplace = True)\n",
        "df.drop(columns=[\"time\",\"fsym\",\"tsym\"])\n",
        "\n",
        "# Resample to 5 minutes\n",
        "df_5min = df.resample('5T').agg({\n",
        "    'open': 'first',\n",
        "    'high': 'max',\n",
        "    'low': 'min',\n",
        "    'close': 'last',\n",
        "    'volumefrom': 'sum',\n",
        "    'volumeto' : 'sum'\n",
        "})\n",
        "\n",
        "\n",
        "# Resample to 15 minutes\n",
        "df_15min = df.resample('15T').agg({\n",
        "    'open': 'first',\n",
        "    'high': 'max',\n",
        "    'low': 'min',\n",
        "    'close': 'last',\n",
        "    'volumefrom': 'sum',\n",
        "    'volumeto' : 'sum'\n",
        "})\n",
        "\n",
        "\n",
        "# Resample to 1 hour\n",
        "df_1h = df.resample('1H').agg({\n",
        "    'open': 'first',\n",
        "    'high': 'max',\n",
        "    'low': 'min',\n",
        "    'close': 'last',\n",
        "    'volumefrom': 'sum',\n",
        "    'volumeto' : 'sum'\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "# Resample to 6 hours\n",
        "df_6h = df.resample('6H').agg({\n",
        "    'open': 'first',\n",
        "    'high': 'max',\n",
        "    'low': 'min',\n",
        "    'close': 'last',\n",
        "    'volumefrom': 'sum',\n",
        "    'volumeto' : 'sum'\n",
        "})\n",
        "\n",
        "\n",
        "# Resample to 6 hours\n",
        "df_1d = df.resample('1D').agg({\n",
        "    'open': 'first',\n",
        "    'high': 'max',\n",
        "    'low': 'min',\n",
        "    'close': 'last',\n",
        "    'volumefrom': 'sum',\n",
        "    'volumeto' : 'sum'\n",
        "})\n",
        "\n",
        "# Now df_5min, df_15min, df_20min, df_1h, df_3h, df_6h are your aggregated DataFrames\n",
        "df_list = [df_5min, df_15min, df_1h, df_6h, df_1d]"
      ],
      "metadata": {
        "id": "NA0_7wiYNuxs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gBuH7p-SEhq_"
      },
      "outputs": [],
      "source": [
        "#GLOBALS :\n",
        "\n",
        "market_impact = 0.015\n",
        "\n",
        "class Actions(int, Enum):\n",
        "    DOUBLE_SELL = 0\n",
        "    SELL = 1\n",
        "    HOLD = 2\n",
        "    BUY = 3\n",
        "    DOUBLE_BUY = 4\n",
        "    CLOSE_ALL = 5\n",
        "    COMBO_BUY = 6\n",
        "\n",
        "\n",
        "class Positions:\n",
        "    SHORT = -1\n",
        "    FLAT = 0\n",
        "    LONG = 1\n",
        "    COUNTER = 0\n",
        "\n",
        "\n",
        "class Ledgers:\n",
        "    ACTIVE_LONG = {\n",
        "        \"Entry Price\" : [],\n",
        "        \"Current Price\" : [],\n",
        "        \"Dollar Profit\" : [],\n",
        "        \"% Return\" : []\n",
        "    }\n",
        "    ACTIVE_SHORT = {\n",
        "        \"Entry Price\" : [],\n",
        "        \"Current Price\" : [],\n",
        "        \"Dollar Profit\" : [],\n",
        "        \"% Return\" : []\n",
        "    }\n",
        "    HIST = {\n",
        "        \"Entry Price\" : [],\n",
        "        \"Action Type\" : [],\n",
        "        \"Position Type\" : [],\n",
        "        \"Dollar Profit, Realized\"   : 0,\n",
        "        \"Dollar Profit, Unrealized\" : 0,\n",
        "        \"Time Index\" : []\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "49Sc2MXqEhrA"
      },
      "outputs": [],
      "source": [
        "# Baseline Models\n",
        "\n",
        "class RandomWalk():\n",
        "    def __init__(self):\n",
        "        self.action_idx_lst = []\n",
        "\n",
        "    def random_selection(self):\n",
        "        if Positions.COUNTER == 0:\n",
        "            self.action_idx_lst = [0, 1, 3, 4, 6]\n",
        "            return random.choice(self.action_idx_lst)\n",
        "        elif abs(Positions.COUNTER) > 8:\n",
        "            self.action_idx_lst = 5\n",
        "            return self.action_idx_lst\n",
        "        else:\n",
        "            return random.choice(self.action_idx_lst)\n",
        "\n",
        "\n",
        "class MomentumFollowing():\n",
        "    def __init__(self):\n",
        "        self.action_idx_lst = []\n",
        "\n",
        "    def trend_selection(self, data, cut_out):\n",
        "        if abs(Positions.COUNTER) > 10:\n",
        "            return 5 #close all\n",
        "        # if data[0][0][0] > data[0][0][cut_out]:\n",
        "        curr_price = data[0][0][-1]\n",
        "        threshold = np.mean(float(data[0][0][-cut_out]) + float(data[0][0][-(cut_out+1)]) +\n",
        "                            float(data[0][0][-(cut_out+2)]) + float(data[0][0][-(cut_out+3)]))\n",
        "        if curr_price > threshold:   #if the latest data is bigger than 5 timestamps old data\n",
        "            if Positions.COUNTER < 0 :\n",
        "                return 5\n",
        "            else:\n",
        "                self.action_idx_lst = [3,4,6]\n",
        "                return random.choice(self.action_idx_lst)\n",
        "        elif curr_price < threshold:\n",
        "            if Positions.COUNTER > 0:\n",
        "                return 5\n",
        "            else:\n",
        "                self.action_idx_lst = [0, 1]\n",
        "                return random.choice(self.action_idx_lst)\n",
        "        else:\n",
        "            return 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "z9DZt54cEhrA"
      },
      "outputs": [],
      "source": [
        "# Convolutional DQN\n",
        "class ConvDQN(nn.Module):\n",
        "    def __init__(self, seq_len_in, actions_n, kernel_size=8):\n",
        "        super(ConvDQN, self).__init__()\n",
        "        n_filters = 64\n",
        "        max_pool_kernel = 2\n",
        "        self.conv1 = nn.Conv1d(1, n_filters, kernel_size)\n",
        "        self.maxPool = nn.MaxPool1d(max_pool_kernel, stride=1)\n",
        "        self.LRelu = nn.LeakyReLU()\n",
        "        self.conv2 = nn.Conv1d(n_filters, n_filters, kernel_size // 2)\n",
        "\n",
        "        self.hidden_dim = n_filters * (\n",
        "                            (\n",
        "                                (\n",
        "                                    (seq_len_in - kernel_size + 1) -\n",
        "                                    max_pool_kernel + 1) -\n",
        "                                    kernel_size // 2 + 1) -\n",
        "                                    max_pool_kernel + 1)\n",
        "\n",
        "        self.out_layer = nn.Linear(self.hidden_dim, actions_n)\n",
        "\n",
        "    def forward(self, x):\n",
        "        c1_out = self.conv1(x)\n",
        "        max_pool_1 = self.maxPool(self.LRelu(c1_out))\n",
        "        c2_out = self.conv2(max_pool_1)\n",
        "        max_pool_2 = self.maxPool(self.LRelu(c2_out))\n",
        "        # print(\"c1_out:\\t%s\"%str(c1_out.shape))\n",
        "        # print(\"max_pool_1:\\t%s\"%str(max_pool_1.shape))\n",
        "        # print(\"c2_out:\\t%s\"%str(c2_out.shape))\n",
        "        # print(\"max_pool_2:\\t%s\"%str(max_pool_2.shape))\n",
        "        # print(self.hidden_dim)\n",
        "        max_pool_2 = max_pool_2.view(-1, self.hidden_dim)\n",
        "        # print(\"max_pool_2_view:\\t%s\"%str(max_pool_2.shape))\n",
        "\n",
        "        return self.LRelu(self.out_layer(max_pool_2))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvDuelingDQN(nn.Module):\n",
        "    def __init__(self, seq_len_in, actions_n, kernel_size=8):\n",
        "        super(ConvDuelingDQN, self).__init__()\n",
        "        n_filters = 64\n",
        "        max_pool_kernel = 2\n",
        "        self.conv1 = nn.Conv1d(1, n_filters, kernel_size)\n",
        "        self.maxPool = nn.MaxPool1d(max_pool_kernel, stride=1)\n",
        "        self.LRelu = nn.LeakyReLU()\n",
        "        self.conv2 = nn.Conv1d(n_filters, n_filters, kernel_size // 2)\n",
        "\n",
        "        self.advantage_hidden_dim = n_filters * (\n",
        "                            (\n",
        "                                (\n",
        "                                    (seq_len_in - kernel_size + 1) -\n",
        "                                    max_pool_kernel + 1) -\n",
        "                                    kernel_size // 2 + 1) -\n",
        "                                    max_pool_kernel + 1)\n",
        "\n",
        "        self.value_hidden_dim = n_filters * (\n",
        "                            (\n",
        "                                (\n",
        "                                    (seq_len_in - kernel_size + 1) -\n",
        "                                    max_pool_kernel + 1) -\n",
        "                                    kernel_size // 2 + 1) -\n",
        "                                    max_pool_kernel + 1)\n",
        "\n",
        "        self.advantage_layer = nn.Linear(self.advantage_hidden_dim, actions_n)\n",
        "        self.value_layer = nn.Linear(self.value_hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        c1_out = self.conv1(x)\n",
        "        max_pool_1 = self.maxPool(self.LRelu(c1_out))\n",
        "        c2_out = self.conv2(max_pool_1)\n",
        "        max_pool_2 = self.maxPool(self.LRelu(c2_out))\n",
        "        max_pool_2 = max_pool_2.view(-1, self.advantage_hidden_dim)\n",
        "\n",
        "        advantage = self.advantage_layer(max_pool_2)\n",
        "        value = self.value_layer(max_pool_2).expand_as(advantage)\n",
        "        q_values = value + (advantage - advantage.mean(1, keepdim=True))\n",
        "        #print(q_values)\n",
        "\n",
        "        return q_values\n"
      ],
      "metadata": {
        "id": "0xdMaNkWKdzd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Linear Dueling\n",
        "class LinearDuelingDQN(nn.Module):\n",
        "    def __init__(self, input_dim, num_actions):\n",
        "        super(LinearDuelingDQN, self).__init__()\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        '''self.feature = nn.Sequential(\n",
        "            nn.Linear(input_dim, 120),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(120, 120),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "\n",
        "        self.advantage_stream = nn.Sequential(\n",
        "            nn.Linear(120, 120),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(120, num_actions)\n",
        "        )\n",
        "\n",
        "        self.value_stream = nn.Sequential(\n",
        "            nn.Linear(120, 120),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(120, 1)\n",
        "        )'''\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc_value = nn.Linear(64, 256)\n",
        "        self.fc_adv = nn.Linear(64, 256)\n",
        "\n",
        "        self.value = nn.Linear(256, 1)\n",
        "        self.adv = nn.Linear(256, self.num_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''x = self.feature(x)\n",
        "        advantage = self.advantage_stream(x)\n",
        "        value = self.value_stream(x)\n",
        "        # Ensure the shapes are compatible for addition and mean calculation\n",
        "        return value + advantage - advantage.mean(dim=1, keepdim=True)'''\n",
        "\n",
        "        y = self.relu(self.fc1(x))\n",
        "        value = self.relu(self.fc_value(y))\n",
        "        adv = self.relu(self.fc_adv(y))\n",
        "\n",
        "        value = self.value(value)\n",
        "        adv = self.adv(adv)\n",
        "\n",
        "        advAverage = torch.mean(adv, dim=1, keepdim=True)\n",
        "        Q = value + adv - advAverage\n",
        "        #print(Q.squeeze(1))\n",
        "\n",
        "        return Q.squeeze(1)\n"
      ],
      "metadata": {
        "id": "3U8se_3tPy_U"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0cJ_8yG-EhrB"
      },
      "outputs": [],
      "source": [
        "#UTIL FUNCTIONS\n",
        "\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward') )\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "\n",
        "def print_stats(model, c_return, t):\n",
        "    c_return = np.array(c_return).flatten()\n",
        "    t.add_row([str(model), \"%.2f\" % np.mean(c_return), \"%.2f\" % np.amax(c_return), \"%.2f\" % np.amin(c_return),\n",
        "               \"%.2f\" % np.std(c_return)])\n",
        "\n",
        "def plot_pnl(name, cum_returns, slice):\n",
        "    \"\"\" NB. cum_returns must be 2-dim \"\"\"\n",
        "    # Mean\n",
        "    M = np.mean(np.array(cum_returns), axis=0)\n",
        "    # std dev\n",
        "    S = np.std(np.array(cum_returns), axis=0)\n",
        "    # upper and lower limit of confidence intervals\n",
        "    LL = M - 0.95 * S\n",
        "    UL = M + 0.95 * S\n",
        "\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(20, 5))  # 1 row, 2 columns\n",
        "\n",
        "    axs[0].plot(range(len(M)), M, linewidth=2)  # Plot the mean curve on the first subplot\n",
        "    axs[0].fill_between(range(len(M)), LL, UL, color='b', alpha=.2)  # Fill between for the first subplot\n",
        "    axs[0].grid(True)\n",
        "    axs[0].set_xlabel(\"Trading Instant (h)\")\n",
        "    axs[0].set_ylabel(\"Return\")\n",
        "    axs[0].legend(['Cumulative Average Return (%)'], loc='upper left')\n",
        "\n",
        "    # axs[1].plot(x=slice.index, y = slice)\n",
        "    axs[1].plot(slice)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4Z07Y-j3EhrB"
      },
      "outputs": [],
      "source": [
        "def transform(position: Positions, action: int, state : float, index) -> Any: #Returns : Position, Realized Profits, Unrealized Profits, Closing (False)/Opening(True) a Trade\n",
        "\n",
        "    \"\"\"\n",
        "    If the 'portfolio' (Ledger) is long more than 1 one unit\n",
        "    We use FIFO (first in first out) to remove the earliest Long in the Active_Long ledger / short in the Active_Short ledger\n",
        "    \"\"\"\n",
        "\n",
        "    # fees = 0.0033 * state\n",
        "    fees = 5\n",
        "    realized_profit = 0\n",
        "    realized_profit_0 = 0\n",
        "    realized_profit_1 = 0\n",
        "\n",
        "    #Updating Dollar Profits for each Ledger:\n",
        "    #1. Update the State:\n",
        "    Ledgers.ACTIVE_LONG[\"Current Price\"] = [state] * len(Ledgers.ACTIVE_LONG[\"Entry Price\"])\n",
        "    Ledgers.ACTIVE_SHORT[\"Current Price\"] = [state] * len(Ledgers.ACTIVE_SHORT[\"Entry Price\"])\n",
        "    #2. Update Profit Figures:\n",
        "    Ledgers.ACTIVE_LONG[\"Dollar Profit\"] = [current - entry for entry, current in zip(Ledgers.ACTIVE_LONG[\"Entry Price\"], Ledgers.ACTIVE_LONG[\"Current Price\"])]\n",
        "    Ledgers.ACTIVE_SHORT[\"Dollar Profit\"] = [entry - current for entry, current in zip(Ledgers.ACTIVE_SHORT[\"Entry Price\"], Ledgers.ACTIVE_SHORT[\"Current Price\"])]\n",
        "    Ledgers.ACTIVE_LONG[\"% Return\"] = [((current - entry)/entry+1) for entry, current in zip(Ledgers.ACTIVE_LONG[\"Entry Price\"], Ledgers.ACTIVE_LONG[\"Current Price\"])]\n",
        "    Ledgers.ACTIVE_SHORT[\"% Return\"] = [(1-(current - entry)/entry) for entry, current in zip(Ledgers.ACTIVE_SHORT[\"Entry Price\"], Ledgers.ACTIVE_SHORT[\"Current Price\"])]\n",
        "\n",
        "    if action == Actions.HOLD:\n",
        "        Ledgers.HIST['Entry Price'].append(state)\n",
        "        Ledgers.HIST['Action Type'].append(int(action))\n",
        "        Ledgers.HIST['Position Type'].append(position)\n",
        "        Ledgers.HIST['Dollar Profit, Unrealized'] = sum(Ledgers.ACTIVE_LONG['Dollar Profit']) + sum(Ledgers.ACTIVE_SHORT['Dollar Profit'])\n",
        "        Ledgers.HIST['Time Index'].append(index)\n",
        "        return position, Ledgers.HIST['Dollar Profit, Realized'], Ledgers.HIST['Dollar Profit, Unrealized'], False\n",
        "\n",
        "    elif action == Actions.BUY:\n",
        "        Ledgers.HIST['Time Index'].append(index)\n",
        "        #Update the counter\n",
        "        Positions.COUNTER += 1\n",
        "        if position == Positions.SHORT:  #CLOSING A POSITION BECAUSE WE ARE IN THE SHORT\n",
        "            #Save the profit of the earliest Long position & update profits positions\n",
        "            realized_profit = Ledgers.ACTIVE_SHORT['Dollar Profit'][0]\n",
        "            Ledgers.HIST['Dollar Profit, Realized'] += realized_profit - fees\n",
        "\n",
        "            #Remove the earliest position : first in, first out\n",
        "            Ledgers.ACTIVE_SHORT = {key: value[1:] for key, value in Ledgers.ACTIVE_SHORT.items()}\n",
        "\n",
        "            #Update Unrealized Profits\n",
        "            Ledgers.HIST['Dollar Profit, Unrealized'] = sum(Ledgers.ACTIVE_LONG['Dollar Profit']) + sum(Ledgers.ACTIVE_SHORT['Dollar Profit'])\n",
        "\n",
        "            if Positions.COUNTER < 0:\n",
        "                Ledgers.HIST['Entry Price'].append(state)\n",
        "                Ledgers.HIST['Action Type'].append(int(action))\n",
        "                Ledgers.HIST['Position Type'].append(Positions.SHORT)\n",
        "\n",
        "                # print(Ledgers.ACTIVE_SHORT['Entry Price'])\n",
        "                assert len(Ledgers.ACTIVE_SHORT['Entry Price']) > 0 #check\n",
        "\n",
        "                return Positions.SHORT, Ledgers.HIST['Dollar Profit, Realized'], Ledgers.HIST['Dollar Profit, Unrealized'], False\n",
        "\n",
        "            elif Positions.COUNTER == 0:\n",
        "                Ledgers.HIST['Entry Price'].append(state)\n",
        "                Ledgers.HIST['Action Type'].append(int(action))\n",
        "                Ledgers.HIST['Position Type'].append(Positions.FLAT)\n",
        "\n",
        "                # print(Ledgers.ACTIVE_SHORT['Entry Price'])\n",
        "                assert len(Ledgers.ACTIVE_SHORT['Entry Price']) == 0 #check\n",
        "\n",
        "                return Positions.FLAT, Ledgers.HIST['Dollar Profit, Realized'], Ledgers.HIST['Dollar Profit, Unrealized'], False\n",
        "\n",
        "        elif position == Positions.LONG or position == Positions.FLAT:  #OPENING A POSITION BECAUSE WE WERE LONG OR FLAT\n",
        "            Ledgers.ACTIVE_LONG[\"Entry Price\"].append(state)\n",
        "            Ledgers.ACTIVE_LONG[\"Current Price\"].append(state)\n",
        "            Ledgers.ACTIVE_LONG[\"Dollar Profit\"].append(0)\n",
        "            Ledgers.ACTIVE_LONG[\"% Return\"].append(1)\n",
        "\n",
        "            Ledgers.HIST['Entry Price'].append(state)\n",
        "            Ledgers.HIST['Action Type'].append(int(action))\n",
        "            Ledgers.HIST['Position Type'].append(Positions.LONG)\n",
        "            Ledgers.HIST['Dollar Profit, Realized'] -= fees\n",
        "            Ledgers.HIST['Dollar Profit, Unrealized'] = sum(Ledgers.ACTIVE_LONG['Dollar Profit']) + sum(Ledgers.ACTIVE_SHORT['Dollar Profit'])\n",
        "\n",
        "\n",
        "            # print(Ledgers.ACTIVE_SHORT['Entry Price'])\n",
        "            assert len(Ledgers.ACTIVE_SHORT['Entry Price']) == 0\n",
        "\n",
        "            return Positions.LONG, Ledgers.HIST['Dollar Profit, Realized'], Ledgers.HIST['Dollar Profit, Unrealized'], True\n",
        "\n",
        "    elif action == Actions.SELL:\n",
        "        Ledgers.HIST['Time Index'].append(index)\n",
        "        #Update the counter\n",
        "        Positions.COUNTER -= 1\n",
        "        if position == Positions.LONG:  #CLOSING A POSITION BECAUSE WE ARE IN THE LONG\n",
        "            #Save the profit of the earliest Long position & update profits positions\n",
        "            realized_profits = Ledgers.ACTIVE_LONG['Dollar Profit'][0]\n",
        "            Ledgers.HIST['Dollar Profit, Realized'] += realized_profits - fees\n",
        "\n",
        "            #Remove the earliest position : first in, first out\n",
        "            Ledgers.ACTIVE_LONG = {key: value[1:] for key, value in Ledgers.ACTIVE_LONG.items()}\n",
        "\n",
        "            #Update Unrealized Profits & Ledgers.HIST\n",
        "            Ledgers.HIST['Dollar Profit, Unrealized'] = sum(Ledgers.ACTIVE_LONG['Dollar Profit']) + sum(Ledgers.ACTIVE_SHORT['Dollar Profit'])\n",
        "            Ledgers.HIST['Entry Price'].append(state)\n",
        "            Ledgers.HIST['Action Type'].append(int(action))\n",
        "\n",
        "            if Positions.COUNTER > 0:\n",
        "                Ledgers.HIST['Position Type'].append(Positions.LONG)\n",
        "                assert len(Ledgers.ACTIVE_LONG['Entry Price']) > 0 #check\n",
        "\n",
        "                return Positions.LONG, Ledgers.HIST['Dollar Profit, Realized'], Ledgers.HIST['Dollar Profit, Unrealized'], False\n",
        "\n",
        "            elif Positions.COUNTER == 0:\n",
        "                Ledgers.HIST['Position Type'].append(Positions.FLAT)\n",
        "                assert len(Ledgers.ACTIVE_LONG['Entry Price']) == 0 #check\n",
        "\n",
        "                return Positions.FLAT, Ledgers.HIST['Dollar Profit, Realized'], Ledgers.HIST['Dollar Profit, Unrealized'], True\n",
        "\n",
        "        elif position == Positions.SHORT or position == Positions.FLAT:  #OPENING A POSITION BECAUSE WE WERE SHORT OR FLAT\n",
        "            Ledgers.ACTIVE_SHORT[\"Entry Price\"].append(state)\n",
        "            Ledgers.ACTIVE_SHORT[\"Current Price\"].append(state)\n",
        "            Ledgers.ACTIVE_SHORT[\"Dollar Profit\"].append(0)\n",
        "            Ledgers.ACTIVE_SHORT[\"% Return\"].append(1)\n",
        "\n",
        "\n",
        "            Ledgers.HIST['Entry Price'].append(state)\n",
        "            Ledgers.HIST['Action Type'].append(int(action))\n",
        "            Ledgers.HIST['Position Type'].append(Positions.SHORT)\n",
        "            Ledgers.HIST['Dollar Profit, Realized'] -= fees\n",
        "\n",
        "            assert len(Ledgers.ACTIVE_LONG['Entry Price']) == 0\n",
        "\n",
        "            return Positions.SHORT, Ledgers.HIST['Dollar Profit, Realized'], Ledgers.HIST['Dollar Profit, Unrealized'], True\n",
        "\n",
        "    elif action == Actions.DOUBLE_BUY:\n",
        "        Ledgers.HIST['Time Index'].append(index)\n",
        "        #Update the counter\n",
        "        Positions.COUNTER += 2\n",
        "        if position == Positions.SHORT and Positions.COUNTER <= 0:  #CLOSING TWO POSITIONS BECAUSE WE ARE IN THE SHORT\n",
        "            #Save the profit of the earliest Long position & update profits positions\n",
        "            realized_profit_0 = Ledgers.ACTIVE_SHORT['Dollar Profit'][0]\n",
        "            realized_profit_1 = Ledgers.ACTIVE_SHORT['Dollar Profit'][1]\n",
        "            Ledgers.HIST['Dollar Profit, Realized'] += realized_profit_0 + realized_profit_1 - fees * 2\n",
        "\n",
        "            #Remove the earliest position : first in, first out\n",
        "            Ledgers.ACTIVE_SHORT = {key: value[2:] for key, value in Ledgers.ACTIVE_SHORT.items()}\n",
        "\n",
        "            #Update Unrealized Profits\n",
        "            Ledgers.HIST['Dollar Profit, Unrealized'] = sum(Ledgers.ACTIVE_LONG['Dollar Profit']) + sum(Ledgers.ACTIVE_SHORT['Dollar Profit'])\n",
        "\n",
        "\n",
        "            if Positions.COUNTER < 0:\n",
        "                #Updating Hist Ledger\n",
        "                Ledgers.HIST['Entry Price'].append(state)\n",
        "                Ledgers.HIST['Action Type'].append(int(action))\n",
        "                Ledgers.HIST['Position Type'].append(Positions.SHORT)\n",
        "\n",
        "                # print(Ledgers.ACTIVE_SHORT['Entry Price'])\n",
        "                assert len(Ledgers.ACTIVE_SHORT['Entry Price']) > 0 #check\n",
        "\n",
        "                return Positions.SHORT, Ledgers.HIST['Dollar Profit, Realized'], Ledgers.HIST['Dollar Profit, Unrealized'], False\n",
        "\n",
        "            elif Positions.COUNTER == 0:\n",
        "                #Updating Hist Ledger\n",
        "                Ledgers.HIST['Entry Price'].append(state)\n",
        "                Ledgers.HIST['Action Type'].append(int(action))\n",
        "                Ledgers.HIST['Position Type'].append(Positions.FLAT)\n",
        "\n",
        "                # print(Ledgers.ACTIVE_SHORT['Entry Price'])\n",
        "                assert len(Ledgers.ACTIVE_SHORT['Entry Price']) == 0 #check\n",
        "\n",
        "                return Positions.FLAT, Ledgers.HIST['Dollar Profit, Realized'], Ledgers.HIST['Dollar Profit, Unrealized'], False\n",
        "\n",
        "        else:  #OPENING A POSITION BECAUSE WE WERE LONG OR FLAT\n",
        "\n",
        "            if Positions.COUNTER - 2 == -1: # that means we were Short 1 unit before and must match it / close it with a Long in FIFO mode\n",
        "                assert len(Ledgers.ACTIVE_SHORT['Entry Price']) == 1\n",
        "                #Closing the last Short position:\n",
        "                #Saving the earliest profit\n",
        "                realized_profit = Ledgers.ACTIVE_SHORT['Dollar Profit'][0]\n",
        "                Ledgers.HIST['Dollar Profit, Realized'] += realized_profit - fees\n",
        "                #Remove the earliest position : first in, first out\n",
        "                Ledgers.ACTIVE_SHORT = {key: value[1:] for key, value in Ledgers.ACTIVE_SHORT.items()}\n",
        "\n",
        "            else: #Opening the first Long\n",
        "                Ledgers.ACTIVE_LONG[\"Entry Price\"].append(state)\n",
        "                Ledgers.ACTIVE_LONG[\"Current Price\"].append(state)\n",
        "                Ledgers.ACTIVE_LONG[\"Dollar Profit\"].append(0)\n",
        "                Ledgers.ACTIVE_LONG[\"% Return\"].append(1)\n",
        "\n",
        "            #Open the second Long\n",
        "            Ledgers.ACTIVE_LONG[\"Entry Price\"].append(state)\n",
        "            Ledgers.ACTIVE_LONG[\"Current Price\"].append(state)\n",
        "            Ledgers.ACTIVE_LONG[\"Dollar Profit\"].append(0)\n",
        "            Ledgers.ACTIVE_LONG[\"% Return\"].append(1)\n",
        "\n",
        "            Ledgers.HIST['Entry Price'].append(state)\n",
        "            Ledgers.HIST['Action Type'].append(int(action))\n",
        "            Ledgers.HIST['Position Type'].append(Positions.LONG)\n",
        "            Ledgers.HIST['Dollar Profit, Realized'] -= fees * 2\n",
        "            Ledgers.HIST['Dollar Profit, Unrealized'] = sum(Ledgers.ACTIVE_LONG['Dollar Profit']) + sum(Ledgers.ACTIVE_SHORT['Dollar Profit'])\n",
        "\n",
        "            # print(Ledgers.ACTIVE_SHORT['Entry Price'])\n",
        "            assert(len(Ledgers.ACTIVE_SHORT['Entry Price'])) == 0\n",
        "\n",
        "            return Positions.LONG, Ledgers.HIST['Dollar Profit, Realized'], Ledgers.HIST['Dollar Profit, Unrealized'], True\n",
        "\n",
        "    elif action == Actions.DOUBLE_SELL:\n",
        "        Ledgers.HIST['Time Index'].append(index)\n",
        "        #Update the counter\n",
        "        Positions.COUNTER -= 2\n",
        "        if position == Positions.LONG and Positions.COUNTER >= 0:  #CLOSING A POSITION BECAUSE WE ARE IN THE LONG\n",
        "            #Save the profit of the earliest Long position & update profits positions\n",
        "            realized_profit_0 = Ledgers.ACTIVE_LONG['Dollar Profit'][0]\n",
        "            realized_profit_1 = Ledgers.ACTIVE_LONG['Dollar Profit'][1]\n",
        "            Ledgers.HIST['Dollar Profit, Realized'] += realized_profit_0 + realized_profit_1 - fees * 2\n",
        "\n",
        "            #Remove the earliest position : first in, first out\n",
        "            Ledgers.ACTIVE_LONG = {key: value[2:] for key, value in Ledgers.ACTIVE_LONG.items()}\n",
        "\n",
        "            #Update Unrealized Profits\n",
        "            Ledgers.HIST['Dollar Profit, Unrealized'] = sum(Ledgers.ACTIVE_LONG['Dollar Profit']) + sum(Ledgers.ACTIVE_SHORT['Dollar Profit'])\n",
        "            Ledgers.HIST['Entry Price'].append(state)\n",
        "            Ledgers.HIST['Action Type'].append(int(action))\n",
        "\n",
        "            if Positions.COUNTER > 0:\n",
        "                Ledgers.HIST['Position Type'].append(Positions.LONG)\n",
        "                assert len(Ledgers.ACTIVE_LONG['Entry Price']) > 0 #check\n",
        "                return Positions.LONG, Ledgers.HIST['Dollar Profit, Realized'], Ledgers.HIST['Dollar Profit, Unrealized'], False\n",
        "\n",
        "            elif Positions.COUNTER == 0:\n",
        "                Ledgers.HIST['Position Type'].append(Positions.FLAT)\n",
        "                assert len(Ledgers.ACTIVE_LONG['Entry Price']) == 0 #check\n",
        "                return Positions.FLAT, Ledgers.HIST['Dollar Profit, Realized'], Ledgers.HIST['Dollar Profit, Unrealized'], False\n",
        "\n",
        "        else:  #OPENING A POSITION BECAUSE WE WERE SHORT OR FLAT\n",
        "            if Positions.COUNTER + 2 == 1: # that means we were Long 1 unit before and must match it / close it with a Short FIFO mode\n",
        "                #First closing:\n",
        "                #Remove the earliest position : first in, first out\n",
        "                realized_profit = Ledgers.ACTIVE_LONG['Dollar Profit'][0]\n",
        "                Ledgers.HIST['Dollar Profit, Realized'] += realized_profit - fees\n",
        "                Ledgers.ACTIVE_LONG = {key: value[1:] for key, value in Ledgers.ACTIVE_LONG.items()}\n",
        "\n",
        "            else: #First Shorting\n",
        "                Ledgers.ACTIVE_SHORT[\"Entry Price\"].append(state)\n",
        "                Ledgers.ACTIVE_SHORT[\"Current Price\"].append(state)\n",
        "                Ledgers.ACTIVE_SHORT[\"Dollar Profit\"].append(0)\n",
        "                Ledgers.ACTIVE_SHORT[\"% Return\"].append(1)\n",
        "\n",
        "            #Second Shorting\n",
        "            Ledgers.ACTIVE_SHORT[\"Entry Price\"].append(state)\n",
        "            Ledgers.ACTIVE_SHORT[\"Current Price\"].append(state)\n",
        "            Ledgers.ACTIVE_SHORT[\"Dollar Profit\"].append(0)\n",
        "            Ledgers.ACTIVE_SHORT[\"% Return\"].append(1)\n",
        "\n",
        "            Ledgers.HIST['Entry Price'].append(state)\n",
        "            Ledgers.HIST['Action Type'].append(int(action))\n",
        "            Ledgers.HIST['Position Type'].append(Positions.SHORT)\n",
        "            Ledgers.HIST['Dollar Profit, Realized'] -= fees * 2\n",
        "            Ledgers.HIST['Dollar Profit, Unrealized'] = sum(Ledgers.ACTIVE_LONG['Dollar Profit']) + sum(Ledgers.ACTIVE_SHORT['Dollar Profit'])\n",
        "\n",
        "            assert len(Ledgers.ACTIVE_LONG['Current Price']) == 0\n",
        "\n",
        "            return Positions.SHORT, Ledgers.HIST['Dollar Profit, Realized'], Ledgers.HIST['Dollar Profit, Unrealized'], True\n",
        "\n",
        "    elif action == Actions.COMBO_BUY:\n",
        "        Ledgers.HIST['Time Index'].append(index)\n",
        "        #Update the counter\n",
        "        Positions.COUNTER += 6\n",
        "        if position == Positions.SHORT and Positions.COUNTER <= 0:  #CLOSING x POSITIONS BECAUSE WE ARE IN THE SHORT\n",
        "            #Save the profit of the earliest Long position & update profits positions\n",
        "            realized_profit_temp = 0\n",
        "            for i in range(6):\n",
        "                realized_profit_temp =+ Ledgers.ACTIVE_SHORT['Dollar Profit'][i]\n",
        "\n",
        "            Ledgers.HIST['Dollar Profit, Realized'] += realized_profit_temp - fees * 6\n",
        "\n",
        "            #Remove the earliest position : first in, first out\n",
        "            # print(Ledgers.ACTIVE_LONG['Current Price'])\n",
        "            Ledgers.ACTIVE_SHORT = {key: value[6:] for key, value in Ledgers.ACTIVE_SHORT.items()}\n",
        "\n",
        "            #Update Unrealized Profits\n",
        "            Ledgers.HIST['Dollar Profit, Unrealized'] = sum(Ledgers.ACTIVE_LONG['Dollar Profit']) + sum(Ledgers.ACTIVE_SHORT['Dollar Profit'])\n",
        "\n",
        "\n",
        "            if Positions.COUNTER < 0:\n",
        "                #Updating Hist Ledger\n",
        "                Ledgers.HIST['Entry Price'].append(state)\n",
        "                Ledgers.HIST['Action Type'].append(int(action))\n",
        "                Ledgers.HIST['Position Type'].append(Positions.SHORT)\n",
        "\n",
        "                # print(Ledgers.ACTIVE_SHORT['Entry Price'])\n",
        "                assert len(Ledgers.ACTIVE_SHORT['Entry Price']) > 0 #check\n",
        "\n",
        "                return Positions.SHORT, Ledgers.HIST['Dollar Profit, Realized'], Ledgers.HIST['Dollar Profit, Unrealized'], False\n",
        "\n",
        "            elif Positions.COUNTER == 0:\n",
        "                #Updating Hist Ledger\n",
        "                Ledgers.HIST['Entry Price'].append(state)\n",
        "                Ledgers.HIST['Action Type'].append(int(action))\n",
        "                Ledgers.HIST['Position Type'].append(Positions.FLAT)\n",
        "\n",
        "                # print(Ledgers.ACTIVE_SHORT['Entry Price'])\n",
        "                assert len(Ledgers.ACTIVE_SHORT['Entry Price']) == 0 #check\n",
        "\n",
        "                return Positions.FLAT, Ledgers.HIST['Dollar Profit, Realized'], Ledgers.HIST['Dollar Profit, Unrealized'], False\n",
        "\n",
        "        else:  #OPENING A POSITION BECAUSE WE WERE LONG OR FLAT\n",
        "            if 0 < Positions.COUNTER < 6: # that means we were Short Position.Count - 6 units before and must match it / close it with a Long in FIFO mode\n",
        "                #Closing the last Short position:\n",
        "                realized_profit_temp = 0\n",
        "                indexer = abs(Positions.COUNTER - 6)\n",
        "                for i in range(indexer):\n",
        "                    realized_profit_temp += Ledgers.ACTIVE_SHORT['Dollar Profit'][i]\n",
        "\n",
        "                Ledgers.HIST['Dollar Profit, Realized'] += realized_profit_temp - fees * indexer\n",
        "                #Remove the earliest position : first in, first out\n",
        "                Ledgers.ACTIVE_SHORT = {key: value[indexer:] for key, value in Ledgers.ACTIVE_SHORT.items()}\n",
        "\n",
        "                #Open the rest of the longs\n",
        "                for i in range(6 - indexer):\n",
        "                    Ledgers.ACTIVE_LONG[\"Entry Price\"].append(state)\n",
        "                    Ledgers.ACTIVE_LONG[\"Current Price\"].append(state)\n",
        "                    Ledgers.ACTIVE_LONG[\"Dollar Profit\"].append(0)\n",
        "                    Ledgers.ACTIVE_LONG[\"% Return\"].append(1)\n",
        "\n",
        "            else: #Updating Long positions\n",
        "                #to improve : simulate order book thinening\n",
        "                for i in range(6):\n",
        "                    Ledgers.ACTIVE_LONG[\"Entry Price\"].append(state)\n",
        "                    Ledgers.ACTIVE_LONG[\"Current Price\"].append(state)\n",
        "                    Ledgers.ACTIVE_LONG[\"Dollar Profit\"].append(0)\n",
        "                    Ledgers.ACTIVE_LONG[\"% Return\"].append(1)\n",
        "\n",
        "\n",
        "            Ledgers.HIST['Entry Price'].append(state)\n",
        "            Ledgers.HIST['Action Type'].append(int(action))\n",
        "            Ledgers.HIST['Position Type'].append(Positions.LONG)\n",
        "            Ledgers.HIST['Dollar Profit, Realized'] -= fees * 2\n",
        "            Ledgers.HIST['Dollar Profit, Unrealized'] = sum(Ledgers.ACTIVE_LONG['Dollar Profit']) + sum(Ledgers.ACTIVE_SHORT['Dollar Profit'])\n",
        "\n",
        "            # print(Ledgers.ACTIVE_SHORT['Entry Price'])\n",
        "            assert(len(Ledgers.ACTIVE_SHORT['Entry Price'])) == 0\n",
        "\n",
        "            return Positions.LONG, Ledgers.HIST['Dollar Profit, Realized'], Ledgers.HIST['Dollar Profit, Unrealized'], True\n",
        "\n",
        "    elif action == Actions.CLOSE_ALL:\n",
        "        Ledgers.HIST['Time Index'].append(index)\n",
        "        Ledgers.HIST['Entry Price'].append(state)\n",
        "        Ledgers.HIST['Action Type'].append(int(action))\n",
        "        Ledgers.HIST['Position Type'].append(Positions.FLAT)\n",
        "        Ledgers.HIST['Dollar Profit, Unrealized'] = sum(Ledgers.ACTIVE_LONG['Dollar Profit']) + sum(Ledgers.ACTIVE_SHORT['Dollar Profit'])\n",
        "        Ledgers.HIST['Dollar Profit, Realized'] += (Ledgers.HIST['Dollar Profit, Unrealized']\n",
        "                                                    - fees * (abs(Positions.COUNTER)))\n",
        "\n",
        "        #Penalty to simulate cost of market impact from market/panic selling:\n",
        "        if Ledgers.HIST['Dollar Profit, Realized'] > 0:\n",
        "            Ledgers.HIST['Dollar Profit, Realized'] *= (1 - market_impact)\n",
        "        else:\n",
        "            Ledgers.HIST['Dollar Profit, Realized'] *= (1 + market_impact)\n",
        "\n",
        "        Ledgers.HIST['Dollar Profit, Unrealized'] = 0\n",
        "\n",
        "        #Resetting:\n",
        "        Ledgers.ACTIVE_LONG = {\n",
        "            \"Entry Price\" : [],\n",
        "            \"Current Price\" : [],\n",
        "            \"Dollar Profit\" : [],\n",
        "            \"% Return\" : []\n",
        "        }\n",
        "        Ledgers.ACTIVE_SHORT = {\n",
        "            \"Entry Price\" : [],\n",
        "            \"Current Price\" : [],\n",
        "            \"Dollar Profit\" : [],\n",
        "            \"% Return\" : []\n",
        "        }\n",
        "\n",
        "        Positions.COUNTER = 0\n",
        "\n",
        "        return Positions.FLAT, Ledgers.HIST['Dollar Profit, Realized'], Ledgers.HIST['Dollar Profit, Unrealized'], False\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "fe4dYsq1EhrD"
      },
      "outputs": [],
      "source": [
        "#ENVIRONMENT\n",
        "\n",
        "class Environment:\n",
        "    def __init__(self, data, aggressive = False):\n",
        "        self.data = data\n",
        "        self.reward_f = \"profit\"\n",
        "        self.aggressive = aggressive\n",
        "        self.reset()\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def reset(self):\n",
        "        self.t = 13\n",
        "        self.done = False\n",
        "        self.aggressive = False\n",
        "        self.agent_pos = Positions.FLAT\n",
        "        Positions.COUNTER = 0\n",
        "        self.init_price = self.data.iloc[0, :]['close']\n",
        "\n",
        "\n",
        "        Ledgers.ACTIVE_LONG = {\n",
        "            \"Entry Price\" : [],\n",
        "            \"Current Price\" : [],\n",
        "            \"Dollar Profit\" : [],\n",
        "            \"% Return\" : []\n",
        "        }\n",
        "\n",
        "        Ledgers.ACTIVE_SHORT = {\n",
        "            \"Entry Price\" : [],\n",
        "            \"Current Price\" : [],\n",
        "            \"Dollar Profit\" : [],\n",
        "            \"% Return\" : []\n",
        "        }\n",
        "\n",
        "        Ledgers.HIST = {\n",
        "            \"Entry Price\" : [],\n",
        "            \"Action Type\" : [],\n",
        "            \"Position Type\" : [],\n",
        "            \"Dollar Profit, Realized\"   : 0,\n",
        "            \"Dollar Profit, Unrealized\" : 0,\n",
        "            \"Time Index\" : []\n",
        "        }\n",
        "\n",
        "        self.agent_init_pos_real = Ledgers.HIST['Dollar Profit, Realized']\n",
        "        self.agent_init_pos_unreal = Ledgers.HIST['Dollar Profit, Realized']\n",
        "        self.agent_pos_total = 0 # realized + unrealized\n",
        "\n",
        "        self.profits            = [0 for e in range(len(self.data))]\n",
        "        self.cumulative_return  = [1 for e in range(len(self.data))]\n",
        "\n",
        "    def get_state(self):\n",
        "        if not self.done:\n",
        "            return torch.tensor([price for price in self.data.iloc[self.t - 13:self.t + 1, :]['close']], device=self.device,\n",
        "                                dtype=torch.float)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def step(self, act):\n",
        "\n",
        "        reward = 0\n",
        "\n",
        "        # GET CURRENT STATE\n",
        "        state = self.data.iloc[self.t, :]['close']\n",
        "        data_idx = self.data.index\n",
        "        index = data_idx[self.t]\n",
        "\n",
        "        # NEW ACTIONS :\n",
        "        # print()\n",
        "        # print()\n",
        "        # print(f'Agent Position, before       : {self.agent_pos}')\n",
        "        # print(f'Position Counter, before     : {Positions.COUNTER}')\n",
        "        # print(f'Action Chosen                : {act}')\n",
        "        self.agent_pos, _realized_profits, _unrealized_profits, _ = transform(self.agent_pos, act, state, index)\n",
        "        # print(f'Agent Position               : {self.agent_pos}')\n",
        "        # print(f'Position Counter             : {Positions.COUNTER}')\n",
        "        # print(f'Agent Realized Profits       : {_realized_profits}')\n",
        "        # print(f'Agent Unrealized Profits     : {_unrealized_profits}')\n",
        "        # print(f'Ledger Hist $ Prof Realized  : {Ledgers.HIST[\"Dollar Profit, Realized\"]}')\n",
        "        # print(f'Ledger Hist $ Prof Unrealized: {Ledgers.HIST[\"Dollar Profit, Unrealized\"]}')\n",
        "        # print(f'Ledger Hist Entry Price      : {Ledgers.HIST[\"Entry Price\"][::-1]}')\n",
        "        # print(f'Ledger Hist Action Type      : {Ledgers.HIST[\"Action Type\"][::-1]}')\n",
        "        # print(f'Ledger Hist Position Type    : {Ledgers.HIST[\"Position Type\"][::-1]}')\n",
        "        # print(f'Ledger Active Long CP        : {Ledgers.ACTIVE_LONG[\"Current Price\"][::-1]}')\n",
        "        # print(f'Ledger Active Long EP        : {Ledgers.ACTIVE_LONG[\"Entry Price\"][::-1]}')\n",
        "        # print(f'Ledger Active Short CP       : {Ledgers.ACTIVE_SHORT[\"Current Price\"][::-1]}')\n",
        "        # print(f'Ledger Active Short EP       : {Ledgers.ACTIVE_SHORT[\"Entry Price\"][::-1]}')\n",
        "\n",
        "        self.profits[self.t] = _realized_profits + _unrealized_profits\n",
        "\n",
        "        self.agent_pos_total += Ledgers.HIST['Dollar Profit, Realized'] + Ledgers.HIST['Dollar Profit, Realized']\n",
        "\n",
        "\n",
        "        self.cumulative_return[self.t] += (reduce(lambda x, y : x * y , Ledgers.ACTIVE_LONG['% Return'], 1)\n",
        "                                           * reduce(lambda x , y : x * y, Ledgers.ACTIVE_SHORT['% Return'], 1))\n",
        "\n",
        "        # COLLECT THE REWARD\n",
        "        reward = 0\n",
        "        risk_free_rate = 0.03\n",
        "        annual_factor = 252\n",
        "\n",
        "        if self.reward_f == \"profit\":\n",
        "            curr_profits = self.profits[self.t]\n",
        "            if curr_profits > 0:\n",
        "                reward = 10\n",
        "            elif curr_profits < 0:\n",
        "                reward = -10\n",
        "            elif curr_profits == 0:\n",
        "                if self.aggressive:\n",
        "                    reward = -2\n",
        "                else:\n",
        "                    reward = 0\n",
        "\n",
        "        if self.agent_pos == Positions.FLAT and (reward > -5): #penalize not trying to improve\n",
        "            reward = -5\n",
        "\n",
        "        # UPDATE THE STATE\n",
        "        self.t += 1\n",
        "\n",
        "        if (self.t == len(self.data) - 1):\n",
        "            self.done = True\n",
        "\n",
        "        return torch.tensor([reward], device=self.device, dtype=torch.float), self.done, torch.tensor([state],\n",
        "                                                                                                 dtype=torch.float)  # reward, done, current_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rWwwCejZEhrD"
      },
      "outputs": [],
      "source": [
        "#AGENT\n",
        "\n",
        "class Agent:\n",
        "\n",
        "    def __init__(self,\n",
        "                 ACTION_NUMBER=len(list(Actions)),\n",
        "                 REPLAY_MEM_SIZE=100,\n",
        "                 BATCH_SIZE=40,\n",
        "                 DISCOUNT=0.98,\n",
        "                 EPS_START=1,\n",
        "                 EPS_END=0.12,\n",
        "                 EPS_STEPS=300,\n",
        "                 LEARNING_RATE=0.001,\n",
        "                 INPUT_DIM=14,\n",
        "                 HIDDEN_DIM=120,\n",
        "                 TARGET_UPDATE=10,\n",
        "                 MODEL = 'ConvDQN'):\n",
        "\n",
        "        self.ACTION_NUMBER = ACTION_NUMBER\n",
        "        self.REPLAY_MEM_SIZE = REPLAY_MEM_SIZE\n",
        "        self.BATCH_SIZE = BATCH_SIZE\n",
        "        self.DISCOUNT = DISCOUNT\n",
        "        self.EPS_START = EPS_START\n",
        "        self.EPS_END = EPS_END\n",
        "        self.EPS_STEPS = EPS_STEPS\n",
        "        self.LEARNING_RATE = LEARNING_RATE\n",
        "        self.INPUT_DIM = INPUT_DIM\n",
        "        self.HIDDEN_DIM = HIDDEN_DIM\n",
        "        self.TARGET_UPDATE = TARGET_UPDATE\n",
        "        self.MODEL = MODEL\n",
        "        self.DOUBLE = False\n",
        "\n",
        "        if self.MODEL == 'LinearDuelingDQN' or self.MODEL == 'ConvDuelingDQN':\n",
        "          self.DOUBLE = True\n",
        "\n",
        "        self.ACTION_LIST = list(Actions)\n",
        "        self.TRAINING = True  # to do not pick random actions during testing\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(\"Agent is using device:\\t\" + str(self.device))\n",
        "\n",
        "        if self.MODEL == 'RandomWalk' :\n",
        "            self.policy_net = RandomWalk()\n",
        "            self.model_name = '_RandomWalk_'\n",
        "\n",
        "        elif self.MODEL == 'MomentumFollowing':\n",
        "            self.policy_net = MomentumFollowing()\n",
        "            self.model_name = '_MomentumFollowing_'\n",
        "\n",
        "        else:\n",
        "            if self.MODEL == 'ConvDQN':\n",
        "                self.policy_net = ConvDQN(self.INPUT_DIM, self.ACTION_NUMBER).to(self.device)\n",
        "                self.target_net = ConvDQN(self.INPUT_DIM, self.ACTION_NUMBER).to(self.device)\n",
        "                self.model_name = '_ConvDQN_'\n",
        "            elif self.MODEL == 'ConvDuelingDQN':\n",
        "                self.policy_net = ConvDuelingDQN(self.INPUT_DIM, self.ACTION_NUMBER).to(self.device)\n",
        "                self.target_net = ConvDuelingDQN(self.INPUT_DIM, self.ACTION_NUMBER).to(self.device)\n",
        "                self.model_name = '_ConvDuelingDQN_'\n",
        "            elif self.MODEL == 'LinearDuelingDQN':\n",
        "                self.policy_net = LinearDuelingDQN(self.INPUT_DIM, self.ACTION_NUMBER).to(self.device)\n",
        "                self.target_net = LinearDuelingDQN(self.INPUT_DIM, self.ACTION_NUMBER).to(self.device)\n",
        "                self.model_name = '_LinearDuelingDQN_'\n",
        "\n",
        "\n",
        "\n",
        "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "            self.target_net.eval()\n",
        "\n",
        "            self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.LEARNING_RATE)\n",
        "\n",
        "        self.memory = ReplayMemory(self.REPLAY_MEM_SIZE)\n",
        "        self.steps_done = 0\n",
        "        self.training_cumulative_reward = []\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\" the epsilon-greedy action selection\"\"\"\n",
        "        state = state.unsqueeze(0).unsqueeze(1)\n",
        "        # print(f'State shape : {state.shape}')\n",
        "        sample = random.random()\n",
        "        if self.TRAINING:\n",
        "            if self.steps_done > self.EPS_STEPS:\n",
        "                eps_threshold = self.EPS_END\n",
        "            else:\n",
        "                eps_threshold = self.EPS_START\n",
        "        else:\n",
        "            eps_threshold = self.EPS_END\n",
        "\n",
        "        self.steps_done += 1\n",
        "\n",
        "        if self.MODEL == 'ConvDQN' or self.MODEL == 'ConvDuelingDQN' or self.MODEL == 'LinearDuelingDQN':\n",
        "            # [Exploitation] pick the best action according to current Q approx.\n",
        "            if sample > eps_threshold:\n",
        "                with torch.no_grad():\n",
        "                    return torch.tensor([self.policy_net(state).argmax()], device=self.device, dtype=torch.long)\n",
        "\n",
        "            # [Exploration]  pick a random action from the action space\n",
        "            else:\n",
        "                return torch.tensor([random.choice(self.ACTION_LIST)], device=self.device, dtype=torch.long)\n",
        "\n",
        "        else:\n",
        "            if self.MODEL == 'RandomWalk':\n",
        "                temp = self.policy_net.random_selection()\n",
        "                return torch.tensor([temp], device=self.device, dtype=torch.long)\n",
        "                # return self.policy_net.random_selection()\n",
        "            elif self.MODEL == 'MomentumFollowing':\n",
        "                temp = self.policy_net.trend_selection(state, cut_out = 5)\n",
        "                return torch.tensor([temp], device=self.device, dtype=torch.long)\n",
        "                # return self.policy_net.trend_selection(state, cut_out = 5)\n",
        "\n",
        "    def optimize_model(self):\n",
        "        if len(self.memory) < self.BATCH_SIZE:\n",
        "            # it will return without doing nothing if we have not enough data to sample\n",
        "            return\n",
        "        transitions = self.memory.sample(self.BATCH_SIZE)\n",
        "\n",
        "        batch = Transition(*zip(*transitions))\n",
        "\n",
        "        # Compute a mask of non-final states and concatenate the batch elements\n",
        "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=self.device,\n",
        "                                      dtype=torch.bool)\n",
        "        nfns = [s for s in batch.next_state if s is not None]\n",
        "        # nfns = []\n",
        "        # for s in batch.next_state:\n",
        "        #     if s is not None:\n",
        "        #         print(s.shape)\n",
        "        #         nfns.append(s)\n",
        "\n",
        "        non_final_next_states = torch.cat(nfns).view(len(nfns), -1)\n",
        "        non_final_next_states = non_final_next_states.unsqueeze(1)\n",
        "\n",
        "        state_batch = torch.cat(batch.state).view(self.BATCH_SIZE, -1)\n",
        "        state_batch = state_batch.unsqueeze(1)\n",
        "        action_batch = torch.cat(batch.action).view(self.BATCH_SIZE, -1)\n",
        "        reward_batch = torch.cat(batch.reward).view(self.BATCH_SIZE, -1)\n",
        "\n",
        "        # Compute Q(s_t, a)\n",
        "        #print(state_batch.shape)\n",
        "        #print(action_batch.shape)\n",
        "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "        if self.DOUBLE: #for Dueling networks\n",
        "            _, next_state_action = self.policy_net(state_batch).max(1, keepdim=True)\n",
        "\n",
        "            next_state_values = torch.zeros(self.BATCH_SIZE, device=self.device).view(self.BATCH_SIZE, -1)\n",
        "\n",
        "            out = self.target_net(non_final_next_states)\n",
        "            next_state_values[non_final_mask] = out.gather(1, next_state_action[non_final_mask])\n",
        "\n",
        "        if not self.DOUBLE:\n",
        "            # Compute V(s_{t+1}) for all next states.\n",
        "            next_state_values = torch.zeros(self.BATCH_SIZE, device=self.device)\n",
        "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n",
        "            next_state_values = next_state_values.view(self.BATCH_SIZE, -1)\n",
        "\n",
        "        # Compute V(s_{t+1}) for all next states.\n",
        "        next_state_values = torch.zeros(self.BATCH_SIZE, device=self.device)\n",
        "        next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n",
        "        next_state_values = next_state_values.view(self.BATCH_SIZE, -1)\n",
        "\n",
        "        # Compute the expected Q values\n",
        "        expected_state_action_values = (next_state_values * self.DISCOUNT) + reward_batch\n",
        "\n",
        "        # Compute MSE loss\n",
        "        loss = F.mse_loss(state_action_values,\n",
        "                          expected_state_action_values)\n",
        "\n",
        "        # Optimize model\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        for param in self.policy_net.parameters():\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def train(self, env, path, num_episodes=30):\n",
        "        self.TRAINING = True\n",
        "        cumulative_reward = [0 for t in range(num_episodes)]\n",
        "        print(\"Training:\")\n",
        "        if (self.MODEL != 'RandomWalk') and (self.MODEL != 'MomentumFollowing'):\n",
        "            for i_episode in tqdm(range(num_episodes)):\n",
        "                # Initialize the environment and state\n",
        "                env.reset()  # reset the env st it is set at the beginning of the time serie\n",
        "                self.steps_done = 0\n",
        "                state = env.get_state()\n",
        "                for t in range(len(env.data)):  # while not env.done\n",
        "                    # Select and perform an action\n",
        "                    action = self.select_action(state)\n",
        "                    reward, done, _ = env.step(action)\n",
        "\n",
        "                    cumulative_reward[i_episode] += reward.item()\n",
        "\n",
        "                    if done:\n",
        "                        break\n",
        "\n",
        "                    else:\n",
        "                        next_state = env.get_state()\n",
        "\n",
        "                        # Store the transition in memory\n",
        "                        self.memory.push(state, action, next_state, reward)\n",
        "\n",
        "                        # Move to the next state\n",
        "                        state = next_state\n",
        "\n",
        "                        self.optimize_model()\n",
        "\n",
        "                # Update the target network, copying all weights and biases of policy_net\n",
        "                if i_episode % self.TARGET_UPDATE == 0:\n",
        "                    self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "            # save the model\n",
        "            model_name = env.reward_f + self.model_name\n",
        "            count = 0\n",
        "            while os.path.exists(path + model_name):  # avoid overrinding models\n",
        "                count += 1\n",
        "                model_name = model_name + \"_\" + str(count)\n",
        "\n",
        "            torch.save(self.policy_net.state_dict(), path + model_name)\n",
        "\n",
        "        else: #Baseline Models\n",
        "            for i_episode in tqdm(range(num_episodes)):\n",
        "                # Initialize the environment and state\n",
        "                env.reset()  # reset the env st it is set at the beginning of the time serie\n",
        "                self.steps_done = 0\n",
        "                state = env.get_state()\n",
        "\n",
        "                for t in range(len(env.data)):  # while not env.done\n",
        "                    # Select and perform an action\n",
        "                    action = self.select_action(state)\n",
        "                    reward, done, _ = env.step(action)\n",
        "\n",
        "                    cumulative_reward[i_episode] += reward.item()\n",
        "\n",
        "                    if done:\n",
        "                        break\n",
        "\n",
        "                    else:\n",
        "                        next_state = env.get_state()\n",
        "\n",
        "                        # Store the transition in memory\n",
        "                        self.memory.push(state, action, next_state, reward)\n",
        "\n",
        "                        # Move to the next state\n",
        "                        state = next_state\n",
        "\n",
        "        return cumulative_reward\n",
        "\n",
        "    def test(self, env_test, path=None):\n",
        "        self.TRAINING = False\n",
        "        cumulative_reward = [0 for _ in range(len(env_test.data))]\n",
        "        reward_list = [0 for _ in range(len(env_test.data))]\n",
        "\n",
        "        if (self.MODEL != 'RandomWalk') and (self.MODEL != 'MomentumFollowing'):\n",
        "            if self.model_name is None:\n",
        "                pass\n",
        "            elif path is not None:\n",
        "                if re.match(\".*_ConvDQN_.*\", self.model_name) or re.match(\".*ConvDuelingDQN.*\", self.model_name) or re.match(\".*LinearDuelingDQN.*\", self.model_name):\n",
        "                  if re.match(\".*_ConvDQN_.*\", self.model_name):\n",
        "                      self.policy_net = ConvDQN(self.INPUT_DIM, self.ACTION_NUMBER).to(self.device)\n",
        "                  elif re.match(\".*ConvDuelingDQN.*\", self.model_name):\n",
        "                      self.policy_net = ConvDuelingDQN(self.INPUT_DIM, self.ACTION_NUMBER).to(self.device)\n",
        "                  elif re.match(\".*LinearDuelingDQN.*\", self.model_name):\n",
        "                      self.policy_net = LinearDuelingDQN(self.INPUT_DIM, self.ACTION_NUMBER).to(self.device)\n",
        "\n",
        "\n",
        "                  if str(self.device) == \"cuda\":\n",
        "                      self.policy_net.load_state_dict(torch.load(path + \"profit\" + self.model_name))\n",
        "                  else:\n",
        "                      self.policy_net.load_state_dict(torch.load(path + \"profit\" + self.model_name, map_location=torch.device('cpu')))\n",
        "                else:\n",
        "                      raise RuntimeError(\"Please Provide a valid model name or valid path.\")\n",
        "            else:\n",
        "                raise RuntimeError('Path can not be None if model Name is not None.')\n",
        "\n",
        "        env_test.reset()\n",
        "        state = env_test.get_state()\n",
        "        for t in tqdm(range(len(env_test.data))):  # while not env.done\n",
        "\n",
        "            # Select and perform an action\n",
        "            action = self.select_action(state)\n",
        "\n",
        "            reward, done, _ = env_test.step(action)\n",
        "\n",
        "            cumulative_reward[t] += reward.item() + cumulative_reward[t - 1 if t - 1 > 0 else 0]\n",
        "            reward_list[t] = reward\n",
        "\n",
        "            next_state = env_test.get_state()\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        return cumulative_reward, reward_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nqDWL9NEhrE",
        "outputId": "a683e3b8-958b-4c29-8c9e-5801c5e977e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "5MIN\n",
            "Agent is using device:\tcuda\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [02:05<00:00,  4.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test nr. 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▎| 185/200 [00:00<00:00, 855.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------+-----------------+----------------+----------------+-----------+\n",
            "|      Trading System     | Avg. Return ($) | Max Return ($) | Min Return ($) | Std. Dev. |\n",
            "+-------------------------+-----------------+----------------+----------------+-----------+\n",
            "| Profit LinearDuelingDQN |      432.18     |    3899.05     |    -2658.94    |  1767.08  |\n",
            "+-------------------------+-----------------+----------------+----------------+-----------+\n",
            "Agent is using device:\tcuda\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [02:18<00:00,  4.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test nr. 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▎| 185/200 [00:00<00:00, 865.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------+-----------------+----------------+----------------+-----------+\n",
            "|     Trading System    | Avg. Return ($) | Max Return ($) | Min Return ($) | Std. Dev. |\n",
            "+-----------------------+-----------------+----------------+----------------+-----------+\n",
            "| Profit ConvDuelingDQN |     -524.48     |     335.53     |    -2208.30    |   574.75  |\n",
            "+-----------------------+-----------------+----------------+----------------+-----------+\n",
            "Agent is using device:\tcuda\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:22<00:00,  1.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test nr. 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▎| 185/200 [00:00<00:00, 800.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------+-----------------+----------------+----------------+-----------+\n",
            "|      Trading System      | Avg. Return ($) | Max Return ($) | Min Return ($) | Std. Dev. |\n",
            "+--------------------------+-----------------+----------------+----------------+-----------+\n",
            "| Profit MomentumFollowing |     -904.53     |     193.70     |    -1919.31    |   660.89  |\n",
            "+--------------------------+-----------------+----------------+----------------+-----------+\n",
            "Agent is using device:\tcuda\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:17<00:00,  1.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test nr. 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▎| 185/200 [00:00<00:00, 1433.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----------------+----------------+----------------+-----------+\n",
            "|   Trading System  | Avg. Return ($) | Max Return ($) | Min Return ($) | Std. Dev. |\n",
            "+-------------------+-----------------+----------------+----------------+-----------+\n",
            "| Profit RandomWalk |     -1379.95    |      0.00      |    -2579.59    |   705.57  |\n",
            "+-------------------+-----------------+----------------+----------------+-----------+\n",
            "Agent is using device:\tcuda\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [01:47<00:00,  3.59s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test nr. 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▎| 185/200 [00:00<00:00, 709.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-----------------+----------------+----------------+-----------+\n",
            "| Trading System | Avg. Return ($) | Max Return ($) | Min Return ($) | Std. Dev. |\n",
            "+----------------+-----------------+----------------+----------------+-----------+\n",
            "| Profit ConvDQN |     -145.04     |     241.97     |    -1494.78    |   244.70  |\n",
            "+----------------+-----------------+----------------+----------------+-----------+\n",
            "\n",
            "\n",
            "15MIN\n",
            "Agent is using device:\tcuda\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [02:03<00:00,  4.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test nr. 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▎| 185/200 [00:00<00:00, 939.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------+-----------------+----------------+----------------+-----------+\n",
            "|      Trading System     | Avg. Return ($) | Max Return ($) | Min Return ($) | Std. Dev. |\n",
            "+-------------------------+-----------------+----------------+----------------+-----------+\n",
            "| Profit LinearDuelingDQN |     -2294.37    |    2602.00     |   -10407.15    |  2461.71  |\n",
            "+-------------------------+-----------------+----------------+----------------+-----------+\n",
            "Agent is using device:\tcuda\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [02:17<00:00,  4.59s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test nr. 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▎| 185/200 [00:00<00:00, 875.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------+-----------------+----------------+----------------+-----------+\n",
            "|     Trading System    | Avg. Return ($) | Max Return ($) | Min Return ($) | Std. Dev. |\n",
            "+-----------------------+-----------------+----------------+----------------+-----------+\n",
            "| Profit ConvDuelingDQN |     2325.23     |    10267.58    |    -1662.92    |  2197.45  |\n",
            "+-----------------------+-----------------+----------------+----------------+-----------+\n",
            "Agent is using device:\tcuda\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:22<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test nr. 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▎| 185/200 [00:00<00:00, 1049.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------+-----------------+----------------+----------------+-----------+\n",
            "|      Trading System      | Avg. Return ($) | Max Return ($) | Min Return ($) | Std. Dev. |\n",
            "+--------------------------+-----------------+----------------+----------------+-----------+\n",
            "| Profit MomentumFollowing |     -2532.45    |     135.64     |    -5658.37    |  1537.16  |\n",
            "+--------------------------+-----------------+----------------+----------------+-----------+\n",
            "Agent is using device:\tcuda\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:17<00:00,  1.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test nr. 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▎| 185/200 [00:00<00:00, 1402.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----------------+----------------+----------------+-----------+\n",
            "|   Trading System  | Avg. Return ($) | Max Return ($) | Min Return ($) | Std. Dev. |\n",
            "+-------------------+-----------------+----------------+----------------+-----------+\n",
            "| Profit RandomWalk |     -1218.42    |     456.13     |    -2753.69    |   912.68  |\n",
            "+-------------------+-----------------+----------------+----------------+-----------+\n",
            "Agent is using device:\tcuda\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [01:48<00:00,  3.62s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test nr. 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▎| 185/200 [00:00<00:00, 934.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-----------------+----------------+----------------+-----------+\n",
            "| Trading System | Avg. Return ($) | Max Return ($) | Min Return ($) | Std. Dev. |\n",
            "+----------------+-----------------+----------------+----------------+-----------+\n",
            "| Profit ConvDQN |     -1687.49    |     746.31     |    -5410.55    |  2331.87  |\n",
            "+----------------+-----------------+----------------+----------------+-----------+\n",
            "\n",
            "\n",
            "1H\n",
            "Agent is using device:\tcuda\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [02:03<00:00,  4.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test nr. 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▎| 185/200 [00:00<00:00, 838.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------+-----------------+----------------+----------------+-----------+\n",
            "|      Trading System     | Avg. Return ($) | Max Return ($) | Min Return ($) | Std. Dev. |\n",
            "+-------------------------+-----------------+----------------+----------------+-----------+\n",
            "| Profit LinearDuelingDQN |     -1761.18    |      0.00      |    -4746.83    |  1234.43  |\n",
            "+-------------------------+-----------------+----------------+----------------+-----------+\n",
            "Agent is using device:\tcuda\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [02:17<00:00,  4.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test nr. 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▎| 185/200 [00:00<00:00, 813.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------+-----------------+----------------+----------------+-----------+\n",
            "|     Trading System    | Avg. Return ($) | Max Return ($) | Min Return ($) | Std. Dev. |\n",
            "+-----------------------+-----------------+----------------+----------------+-----------+\n",
            "| Profit ConvDuelingDQN |     -963.35     |      0.00      |    -2390.98    |   694.70  |\n",
            "+-----------------------+-----------------+----------------+----------------+-----------+\n",
            "Agent is using device:\tcuda\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:22<00:00,  1.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test nr. 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▎| 185/200 [00:00<00:00, 764.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------+-----------------+----------------+----------------+-----------+\n",
            "|      Trading System      | Avg. Return ($) | Max Return ($) | Min Return ($) | Std. Dev. |\n",
            "+--------------------------+-----------------+----------------+----------------+-----------+\n",
            "| Profit MomentumFollowing |     -1519.11    |      0.00      |    -3238.61    |   993.33  |\n",
            "+--------------------------+-----------------+----------------+----------------+-----------+\n",
            "Agent is using device:\tcuda\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:17<00:00,  1.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test nr. 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▎| 185/200 [00:00<00:00, 1460.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----------------+----------------+----------------+-----------+\n",
            "|   Trading System  | Avg. Return ($) | Max Return ($) | Min Return ($) | Std. Dev. |\n",
            "+-------------------+-----------------+----------------+----------------+-----------+\n",
            "| Profit RandomWalk |     -1228.47    |      0.00      |    -2725.21    |   883.73  |\n",
            "+-------------------+-----------------+----------------+----------------+-----------+\n",
            "Agent is using device:\tcuda\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [01:47<00:00,  3.57s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test nr. 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▎| 185/200 [00:00<00:00, 938.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-----------------+----------------+----------------+-----------+\n",
            "| Trading System | Avg. Return ($) | Max Return ($) | Min Return ($) | Std. Dev. |\n",
            "+----------------+-----------------+----------------+----------------+-----------+\n",
            "| Profit ConvDQN |     -1083.14    |      0.00      |    -2293.60    |   770.53  |\n",
            "+----------------+-----------------+----------------+----------------+-----------+\n",
            "\n",
            "\n",
            "6H\n",
            "Agent is using device:\tcuda\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [02:03<00:00,  4.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test nr. 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▎| 185/200 [00:00<00:00, 921.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------+-----------------+----------------+----------------+-----------+\n",
            "|      Trading System     | Avg. Return ($) | Max Return ($) | Min Return ($) | Std. Dev. |\n",
            "+-------------------------+-----------------+----------------+----------------+-----------+\n",
            "| Profit LinearDuelingDQN |     -338.84     |     539.51     |    -1732.46    |   509.73  |\n",
            "+-------------------------+-----------------+----------------+----------------+-----------+\n",
            "Agent is using device:\tcuda\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [02:17<00:00,  4.59s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test nr. 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▎| 185/200 [00:00<00:00, 801.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------+-----------------+----------------+----------------+-----------+\n",
            "|     Trading System    | Avg. Return ($) | Max Return ($) | Min Return ($) | Std. Dev. |\n",
            "+-----------------------+-----------------+----------------+----------------+-----------+\n",
            "| Profit ConvDuelingDQN |     -1127.94    |      0.00      |    -2660.31    |   678.71  |\n",
            "+-----------------------+-----------------+----------------+----------------+-----------+\n",
            "Agent is using device:\tcuda\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:23<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test nr. 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▎| 185/200 [00:00<00:00, 1067.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------+-----------------+----------------+----------------+-----------+\n",
            "|      Trading System      | Avg. Return ($) | Max Return ($) | Min Return ($) | Std. Dev. |\n",
            "+--------------------------+-----------------+----------------+----------------+-----------+\n",
            "| Profit MomentumFollowing |     -453.38     |     404.93     |    -1648.13    |   474.40  |\n",
            "+--------------------------+-----------------+----------------+----------------+-----------+\n",
            "Agent is using device:\tcuda\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:17<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test nr. 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▎| 185/200 [00:00<00:00, 1382.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----------------+----------------+----------------+-----------+\n",
            "|   Trading System  | Avg. Return ($) | Max Return ($) | Min Return ($) | Std. Dev. |\n",
            "+-------------------+-----------------+----------------+----------------+-----------+\n",
            "| Profit RandomWalk |     -1730.07    |      0.00      |    -3565.08    |  1089.33  |\n",
            "+-------------------+-----------------+----------------+----------------+-----------+\n",
            "Agent is using device:\tcuda\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [01:48<00:00,  3.61s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test nr. 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▎| 185/200 [00:00<00:00, 897.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-----------------+----------------+----------------+-----------+\n",
            "| Trading System | Avg. Return ($) | Max Return ($) | Min Return ($) | Std. Dev. |\n",
            "+----------------+-----------------+----------------+----------------+-----------+\n",
            "| Profit ConvDQN |      15.35      |     802.51     |    -1821.20    |   524.18  |\n",
            "+----------------+-----------------+----------------+----------------+-----------+\n",
            "\n",
            "\n",
            "1D\n",
            "Agent is using device:\tcuda\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [02:04<00:00,  4.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test nr. 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▎| 185/200 [00:00<00:00, 910.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------+-----------------+----------------+----------------+-----------+\n",
            "|      Trading System     | Avg. Return ($) | Max Return ($) | Min Return ($) | Std. Dev. |\n",
            "+-------------------------+-----------------+----------------+----------------+-----------+\n",
            "| Profit LinearDuelingDQN |    -17239.02    |    23179.56    |   -88673.97    |  22796.36 |\n",
            "+-------------------------+-----------------+----------------+----------------+-----------+\n",
            "Agent is using device:\tcuda\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [02:17<00:00,  4.59s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test nr. 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▎| 185/200 [00:00<00:00, 774.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------+-----------------+----------------+----------------+-----------+\n",
            "|     Trading System    | Avg. Return ($) | Max Return ($) | Min Return ($) | Std. Dev. |\n",
            "+-----------------------+-----------------+----------------+----------------+-----------+\n",
            "| Profit ConvDuelingDQN |    -43162.95    |   391430.81    |   -700466.11   | 244463.84 |\n",
            "+-----------------------+-----------------+----------------+----------------+-----------+\n",
            "Agent is using device:\tcuda\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:22<00:00,  1.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test nr. 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▎| 185/200 [00:00<00:00, 753.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------+-----------------+----------------+----------------+-----------+\n",
            "|      Trading System      | Avg. Return ($) | Max Return ($) | Min Return ($) | Std. Dev. |\n",
            "+--------------------------+-----------------+----------------+----------------+-----------+\n",
            "| Profit MomentumFollowing |     7107.32     |    28188.10    |   -21889.11    |  12228.84 |\n",
            "+--------------------------+-----------------+----------------+----------------+-----------+\n",
            "Agent is using device:\tcuda\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:17<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test nr. 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▎| 185/200 [00:00<00:00, 1371.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----------------+----------------+----------------+-----------+\n",
            "|   Trading System  | Avg. Return ($) | Max Return ($) | Min Return ($) | Std. Dev. |\n",
            "+-------------------+-----------------+----------------+----------------+-----------+\n",
            "| Profit RandomWalk |    -12340.76    |    8631.09     |   -25933.46    |  7714.79  |\n",
            "+-------------------+-----------------+----------------+----------------+-----------+\n",
            "Agent is using device:\tcuda\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [01:53<00:00,  3.78s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test nr. 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▎| 185/200 [00:00<00:00, 878.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-----------------+----------------+----------------+-----------+\n",
            "| Trading System | Avg. Return ($) | Max Return ($) | Min Return ($) | Std. Dev. |\n",
            "+----------------+-----------------+----------------+----------------+-----------+\n",
            "| Profit ConvDQN |    116148.74    |   362024.94    |   -12534.56    |  73734.17 |\n",
            "+----------------+-----------------+----------------+----------------+-----------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "analytic_dic = {\n",
        "    \"5MIN\" : {\n",
        "        'LinearDuelingDQN' : {\n",
        "            \"Time Index\" : None,\n",
        "            \"Price\" : None,\n",
        "            \"Profit\" : None,\n",
        "            \"Action Type\" : None,\n",
        "            \"Position Type\" : None,\n",
        "            \"Time Action Idx\" : None\n",
        "        },\n",
        "        'ConvDuelingDQN' : {\n",
        "            \"Time Index\" : None,\n",
        "            \"Price\" : None,\n",
        "            \"Profit\" : None,\n",
        "            \"Action Type\" : None,\n",
        "            \"Position Type\" : None,\n",
        "            \"Time Action Idx\" : None\n",
        "        },\n",
        "        'MomentumFollowing' : {\n",
        "            \"Time Index\" : None,\n",
        "            \"Price\" : None,\n",
        "            \"Profit\" : None,\n",
        "            \"Action Type\" : None,\n",
        "            \"Position Type\" : None,\n",
        "            \"Time Action Idx\" : None\n",
        "        },\n",
        "        'RandomWalk' : {\n",
        "            \"Time Index\" : None,\n",
        "            \"Price\" : None,\n",
        "            \"Profit\" : None,\n",
        "            \"Action Type\" : None,\n",
        "            \"Position Type\" : None,\n",
        "            \"Time Action Idx\" : None\n",
        "        },\n",
        "        'ConvDQN' : {\n",
        "            \"Time Index\" : None,\n",
        "            \"Price\" : None,\n",
        "            \"Profit\" : None,\n",
        "            \"Action Type\" : None,\n",
        "            \"Position Type\" : None,\n",
        "            \"Time Action Idx\" : None\n",
        "        },\n",
        "    },\n",
        "    \"15MIN\" : {\n",
        "        'LinearDuelingDQN' : {\n",
        "            \"Time Index\" : None,\n",
        "            \"Price\" : None,\n",
        "            \"Profit\" : None,\n",
        "            \"Action Type\" : None,\n",
        "            \"Position Type\" : None,\n",
        "            \"Time Action Idx\" : None\n",
        "        },\n",
        "        'ConvDuelingDQN' : {\n",
        "            \"Time Index\" : None,\n",
        "            \"Price\" : None,\n",
        "            \"Profit\" : None,\n",
        "            \"Action Type\" : None,\n",
        "            \"Position Type\" : None,\n",
        "            \"Time Action Idx\" : None\n",
        "        },\n",
        "        'MomentumFollowing' : {\n",
        "            \"Time Index\" : None,\n",
        "            \"Price\" : None,\n",
        "            \"Profit\" : None,\n",
        "            \"Action Type\" : None,\n",
        "            \"Position Type\" : None,\n",
        "            \"Time Action Idx\" : None\n",
        "        },\n",
        "        'RandomWalk' : {\n",
        "            \"Time Index\" : None,\n",
        "            \"Price\" : None,\n",
        "            \"Profit\" : None,\n",
        "            \"Action Type\" : None,\n",
        "            \"Position Type\" : None,\n",
        "            \"Time Action Idx\" : None\n",
        "        },\n",
        "        'ConvDQN' : {\n",
        "            \"Time Index\" : None,\n",
        "            \"Price\" : None,\n",
        "            \"Profit\" : None,\n",
        "            \"Action Type\" : None,\n",
        "            \"Position Type\" : None,\n",
        "            \"Time Action Idx\" : None\n",
        "        },\n",
        "    },\n",
        "    \"1H\" : {\n",
        "        'LinearDuelingDQN' : {\n",
        "            \"Time Index\" : None,\n",
        "            \"Price\" : None,\n",
        "            \"Profit\" : None,\n",
        "            \"Action Type\" : None,\n",
        "            \"Position Type\" : None,\n",
        "            \"Time Action Idx\" : None\n",
        "        },\n",
        "        'ConvDuelingDQN' : {\n",
        "            \"Time Index\" : None,\n",
        "            \"Price\" : None,\n",
        "            \"Profit\" : None,\n",
        "            \"Action Type\" : None,\n",
        "            \"Position Type\" : None,\n",
        "            \"Time Action Idx\" : None\n",
        "        },\n",
        "        'MomentumFollowing' : {\n",
        "            \"Time Index\" : None,\n",
        "            \"Price\" : None,\n",
        "            \"Profit\" : None,\n",
        "            \"Action Type\" : None,\n",
        "            \"Position Type\" : None,\n",
        "            \"Time Action Idx\" : None\n",
        "        },\n",
        "        'RandomWalk' : {\n",
        "            \"Time Index\" : None,\n",
        "            \"Price\" : None,\n",
        "            \"Profit\" : None,\n",
        "            \"Action Type\" : None,\n",
        "            \"Position Type\" : None,\n",
        "            \"Time Action Idx\" : None\n",
        "        },\n",
        "        'ConvDQN' : {\n",
        "            \"Time Index\" : None,\n",
        "            \"Price\" : None,\n",
        "            \"Profit\" : None,\n",
        "            \"Action Type\" : None,\n",
        "            \"Position Type\" : None,\n",
        "            \"Time Action Idx\" : None\n",
        "        },\n",
        "    },\n",
        "    \"6H\" : {\n",
        "        'LinearDuelingDQN' : {\n",
        "            \"Time Index\" : None,\n",
        "            \"Price\" : None,\n",
        "            \"Profit\" : None,\n",
        "            \"Action Type\" : None,\n",
        "            \"Position Type\" : None,\n",
        "            \"Time Action Idx\" : None\n",
        "        },\n",
        "        'ConvDuelingDQN' : {\n",
        "            \"Time Index\" : None,\n",
        "            \"Price\" : None,\n",
        "            \"Profit\" : None,\n",
        "            \"Action Type\" : None,\n",
        "            \"Position Type\" : None,\n",
        "            \"Time Action Idx\" : None\n",
        "        },\n",
        "        'MomentumFollowing' : {\n",
        "            \"Time Index\" : None,\n",
        "            \"Price\" : None,\n",
        "            \"Profit\" : None,\n",
        "            \"Action Type\" : None,\n",
        "            \"Position Type\" : None,\n",
        "            \"Time Action Idx\" : None\n",
        "        },\n",
        "        'RandomWalk' : {\n",
        "            \"Time Index\" : None,\n",
        "            \"Price\" : None,\n",
        "            \"Profit\" : None,\n",
        "            \"Action Type\" : None,\n",
        "            \"Position Type\" : None,\n",
        "            \"Time Action Idx\" : None\n",
        "        },\n",
        "        'ConvDQN' : {\n",
        "            \"Time Index\" : None,\n",
        "            \"Price\" : None,\n",
        "            \"Profit\" : None,\n",
        "            \"Action Type\" : None,\n",
        "            \"Position Type\" : None,\n",
        "            \"Time Action Idx\" : None\n",
        "        },\n",
        "    },\n",
        "    \"1D\" : {\n",
        "        'LinearDuelingDQN' : {\n",
        "            \"Time Index\" : None,\n",
        "            \"Price\" : None,\n",
        "            \"Profit\" : None,\n",
        "            \"Action Type\" : None,\n",
        "            \"Position Type\" : None,\n",
        "            \"Time Action Idx\" : None\n",
        "        },\n",
        "        'ConvDuelingDQN' : {\n",
        "            \"Time Index\" : None,\n",
        "            \"Price\" : None,\n",
        "            \"Profit\" : None,\n",
        "            \"Action Type\" : None,\n",
        "            \"Position Type\" : None,\n",
        "            \"Time Action Idx\" : None\n",
        "        },\n",
        "        'MomentumFollowing' : {\n",
        "            \"Time Index\" : None,\n",
        "            \"Price\" : None,\n",
        "            \"Profit\" : None,\n",
        "            \"Action Type\" : None,\n",
        "            \"Position Type\" : None,\n",
        "            \"Time Action Idx\" : None\n",
        "        },\n",
        "        'RandomWalk' : {\n",
        "            \"Time Index\" : None,\n",
        "            \"Price\" : None,\n",
        "            \"Profit\" : None,\n",
        "            \"Action Type\" : None,\n",
        "            \"Position Type\" : None,\n",
        "            \"Time Action Idx\" : None\n",
        "        },\n",
        "        'ConvDQN' : {\n",
        "            \"Time Index\" : None,\n",
        "            \"Price\" : None,\n",
        "            \"Profit\" : None,\n",
        "            \"Action Type\" : None,\n",
        "            \"Position Type\" : None,\n",
        "            \"Time Action Idx\" : None\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "def main_():\n",
        "    global analytic_dic\n",
        "    #----------------------------- LOAD DATA ---------------------------------------------------------------------------\n",
        "    path = './'\n",
        "\n",
        "\n",
        "    # ----------------------------- AGENTS INPUT --------------------------------\n",
        "    # ACTION_NUMBER = len(list(Actions))\n",
        "    ACTION_NUMBER = len(list(Actions))\n",
        "    REPLAY_MEM_SIZE = 100\n",
        "    BATCH_SIZE = 10\n",
        "    DISCOUNT = 0.98\n",
        "    EPS_START = 1\n",
        "    EPS_END = 0.12\n",
        "    EPS_STEPS = 100\n",
        "    LEARNING_RATE = 0.001\n",
        "    INPUT_DIM = 14\n",
        "    HIDDEN_DIM = 120\n",
        "    TARGET_UPDATE = 10\n",
        "    N_TEST = 1\n",
        "    TRADING_PERIOD = 1000\n",
        "    MODEL_LIST = ['LinearDuelingDQN','ConvDuelingDQN','MomentumFollowing', 'RandomWalk', 'ConvDQN']\n",
        "\n",
        "        # \"Action Type\" : [],\n",
        "        # \"Position Type\" : [],\n",
        "\n",
        "\n",
        "    df_list_names = [\"5MIN\",\"15MIN\",\"1H\",\"6H\",\"1D\"]\n",
        "    for i, df_ in enumerate(df_list):\n",
        "      print()\n",
        "      print()\n",
        "      print(df_list_names[i])\n",
        "      index = random.randrange(len(df_) - TRADING_PERIOD - 1)\n",
        "      for MODEL in MODEL_LIST:\n",
        "          dqn_agent = Agent(ACTION_NUMBER,\n",
        "                          REPLAY_MEM_SIZE,\n",
        "                          BATCH_SIZE,\n",
        "                          DISCOUNT,\n",
        "                          EPS_START,\n",
        "                          EPS_END,\n",
        "                          EPS_STEPS,\n",
        "                          LEARNING_RATE,\n",
        "                          INPUT_DIM,\n",
        "                          HIDDEN_DIM,\n",
        "                          TARGET_UPDATE,\n",
        "                          MODEL)\n",
        "\n",
        "          train_size = int(TRADING_PERIOD * 0.8)\n",
        "          profit_dqn_return = []\n",
        "\n",
        "          profit_train_env = Environment(df_[index:index + train_size], \"profit\")\n",
        "\n",
        "          # Profit Double DQN\n",
        "          cr_profit_dqn = dqn_agent.train(profit_train_env, path)\n",
        "          profit_train_env.reset()\n",
        "          print()\n",
        "          j = 0\n",
        "          while j < N_TEST:\n",
        "              print(\"Test nr. %s\" % str(i+1))\n",
        "              # index = random.randrange(len(df) - TRADING_PERIOD - 1)\n",
        "\n",
        "              profit_test_env = Environment(df_[index + train_size:index + TRADING_PERIOD], \"profit\")\n",
        "\n",
        "              # Profit Double DQN\n",
        "              cr_profit_dqn_test, _ = dqn_agent.test(profit_test_env , path=path)\n",
        "              profit_dqn_return.append(profit_test_env.profits)\n",
        "              # profit_dqn_return = profit_dqn_return[0][:-1]\n",
        "              del profit_dqn_return[0][-1]\n",
        "\n",
        "              slice_df = df_[index + train_size:index + TRADING_PERIOD]\n",
        "\n",
        "              analytic_dic[df_list_names[i]][MODEL][\"Time Index\"] = slice_df.index\n",
        "              analytic_dic[df_list_names[i]][MODEL][\"Price\"] = slice_df.close\n",
        "              analytic_dic[df_list_names[i]][MODEL][\"Profit\"] = profit_dqn_return\n",
        "              analytic_dic[df_list_names[i]][MODEL][\"Action Type\"] = list(Ledgers.HIST['Action Type'])\n",
        "              analytic_dic[df_list_names[i]][MODEL][\"Entry Price\"] = list(Ledgers.HIST['Entry Price'])\n",
        "              analytic_dic[df_list_names[i]][MODEL][\"Position Type\"] = list(Ledgers.HIST['Position Type'])\n",
        "              analytic_dic[df_list_names[i]][MODEL][\"Time Action Idx\"] = list(Ledgers.HIST['Time Index'])\n",
        "\n",
        "              profit_test_env.reset()\n",
        "\n",
        "\n",
        "              #--------------------------------------- Print Test Stats ---------------------------------------------------------\n",
        "              t = PrettyTable([\"Trading System\", \"Avg. Return ($)\", \"Max Return ($)\", \"Min Return ($)\", \"Std. Dev.\"])\n",
        "              print_stats(f'Profit {MODEL}', profit_dqn_return, t)\n",
        "\n",
        "              print(t)\n",
        "\n",
        "\n",
        "              # plot_pnl(\"Profit C-DQN\", profit_dqn_return, slice)\n",
        "              # print(len(temp_lst))\n",
        "\n",
        "              j += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analytic_dic"
      ],
      "metadata": {
        "id": "6FKqfhbAlKZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save dictionary to a file using pickle\n",
        "with open('analytic_dic.pkl', 'wb') as pickle_file:\n",
        "    pickle.dump(analytic_dic, pickle_file)\n"
      ],
      "metadata": {
        "id": "2IJvd-VbGugG"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Load dictionary from the pickle file\n",
        "with open('analytic_dic.pkl', 'rb') as pickle_file:\n",
        "    reloaded_dict = pickle.load(pickle_file)\n",
        "\n",
        "# Print the reloaded dictionary\n",
        "print(reloaded_dict[\"5MIN\"][\"LinearDuelingDQN\"][\"Action Type\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMvYBJ5hG5Tw",
        "outputId": "df57eb04-0e36-4e34-f6ed-a7c4182da891"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 0, 0, 0, 6, 4, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 5, 1, 0, 0, 0, 0, 5, 0, 0, 0, 1, 0, 1, 0, 0, 6, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 6, 0, 5, 0, 0, 0, 4, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 5, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z-CF_sC4HgOl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}